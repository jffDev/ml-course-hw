{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Credits: materials from this notebook belong to YSDA [Practical DL](https://github.com/yandexdataschool/Practical_DL) course. Special thanks for making them available online.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab assignment №1, part 1\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Matrix differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Так как $x^Tx = <x,x>$ является скалярным произведением и раскладывается на $y=x_{1}^{2} + ... + x_{N}^{2}$, тогда производная равна:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 2x\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Пусть $F(X)$ — дифференцируемая функция по каждому элементу X. Тогда верно, что\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{Tr(F(X)}}{\\partial{X}} = f(X)^{T}\n",
    "$$\n",
    "\n",
    "Тогда получаем, что производная $\\frac{dy}{dA}$ равна:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = B^{T}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выражение $y = x^TAc$ можно представить следующим образом:\n",
    "\n",
    "$y=x^{T}Ac= \\sum \\limits_{i=1}^{N} c_{i} \\sum \\limits_{j=1}^{N} a_{ji}x_{j}$\n",
    "\n",
    "тогда решение будет:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dx^{T}Ac}{dx} = Ac\n",
    "$$\n",
    "\n",
    "и\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = \\frac{dx^{T}Ac}{dA} = xc^{T}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "\\frac{dJ}{dS} = 2A^T(AS - X)\n",
    "$$\n",
    "\n",
    "Полное решение представлено ниже.\n",
    "\n",
    "You may use one of the following approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T) \n",
    "$$ \n",
    "it is easy to derive gradients (you can find it in one of the refs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Second approach\n",
    "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
    "<img src=\"grad.png\">\n",
    "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Third approach\n",
    "And finally we can use chain rule! \n",
    "let $ F = AS $ \n",
    "\n",
    "**Find**\n",
    "$$\n",
    "\\frac{dJ}{dF} =  \n",
    "$$ \n",
    "and \n",
    "$$\n",
    "\\frac{dF}{dS} =  \n",
    "$$ \n",
    "(the shape should be $ NM \\times RM )$.\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} =  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Решение ex.4:\n",
    "\n",
    "$$\n",
    "J = || X - AS ||_2^2 , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M} \\\\\n",
    "$$\n",
    "\n",
    "Тогда\n",
    "\n",
    "$$\n",
    "J = || X - AS ||_2^2 = tr((X - AS)(X - AS)^T) = tr(XX^T - ASX^T - XS^TA^T + ASS^TA^T) = tr(XX^T) - tr(ASX^T) - tr(XS^TA^T) + tr(ASS^TA^T))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dS} = \\frac{d(tr(XX^T - ASX^T - XS^TA^T + ASS^TA^T))}{dS} = \\left(-A^TX -A^TX + \\frac{d(tr(SS^TA^TA))}{dS}\\right) = -2A^TX + 2A^TAS = 2A^T(AS - X)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dS} = 2A^T(AS - X)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. kNN questions\n",
    "Here come the questions from the assignment0_01. Please, refer to the assignment0_01 to get the context of the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)\n",
    "\n",
    "- What in the data is the cause behind the distinctly bright rows?\n",
    "- What causes the columns?\n",
    "\n",
    "*Your Answer:*\n",
    "Возможно это наблюдение(яркость строки) из класса, которого нет в наборе обучающих данных или, по крайней мере, оно сильно отличается от большинства данных тестового набора с точки зрения цвета и находится дальше всех остальных.\n",
    "\n",
    "Для яркости столбцов тренировочного набора ситуация аналогичная, наблюдение находится дальше всех от остальных.\n",
    "\n",
    "Как пример, можно привести белое изображение на черном фоне или наоборот, черное изображение на белом фоне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 2\n",
    "\n",
    "We can also use other distance metrics such as L1 distance.\n",
    "For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$, \n",
    "\n",
    "the mean $\\mu$ across all pixels over all images is $$\\mu=\\frac{1}{nhw}\\sum_{k=1}^n\\sum_{i=1}^{h}\\sum_{j=1}^{w}p_{ij}^{(k)}$$\n",
    "And the pixel-wise mean $\\mu_{ij}$ across all images is \n",
    "$$\\mu_{ij}=\\frac{1}{n}\\sum_{k=1}^np_{ij}^{(k)}.$$\n",
    "The general standard deviation $\\sigma$ and pixel-wise standard deviation $\\sigma_{ij}$ is defined similarly.\n",
    "\n",
    "\n",
    "Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.\n",
    "1. Subtracting the mean $\\mu$ ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu$.)\n",
    "2. Subtracting the per pixel mean $\\mu_{ij}$  ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu_{ij}$.)\n",
    "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$.\n",
    "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$.\n",
    "5. Rotating the coordinate axes of the data.\n",
    "\n",
    "*Your Answer:* 1, 2, 3, 5(в зависимости от постановки задачи, описание ниже)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Your Explanation:*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. *Subtracting the mean $\\mu$ ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu$.)*\n",
    "\n",
    "Вычитание среднего значения не окажет значимого влияния на перфоманс классификатора knn, т.к. в данном случае вычитание приведет только к смещению значений пикселей:\n",
    "\n",
    "$$\n",
    "L_{1}=\\frac{1}{n} \\sum_{i=1}^n ||\\,(x_{test} - \\bar{x}) - (x_{train}^{(i)} - \\bar{x})\\,||_{1} = \\frac{1}{n} \\sum_{i=1}^n ||\\,(x_{test} - \\mu) - (x_{train}^{(i)} - \\mu)\\,||_{1} = ||\\,x_{test} - x_{train}^{(i)}\\,||_{1}\n",
    "\n",
    "$$\n",
    "Следовательно расстояния сохраняются при вычитании среднего значения."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Subtracting the per pixel mean $\\mu_{ij}$  ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu_{ij}$.)\n",
    "\n",
    "Как и в первом случае вычитание среднего значения для каждого пикселя не окажет значимого влияния на перфоманс классификатора knn. Если расписать по $ij$ координатам для k-х изображений, то можно показать отсутствие влияния следующим образом:\n",
    "\n",
    "$$\n",
    "L_{1}=\\frac{1}{nhw}\\sum_{k=1}^n\\sum_{i=1}^{h}\\sum_{j=1}^{w} ||\\,(p_{ij} - \\mu_{ij}) - (q_{ij}^{(k)} - \\mu_{ij})\\,||_{1} = ||\\,p_{ij} - q_{ij}^{(k)}\\,||_{1}\n",
    "$$\n",
    "\n",
    "где $p_{ij}$ и $q_{ij}^{(k)}$ пикселы точек $(i,j)$ тестового и $k$-го тренировочного изображения соответственно.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$.\n",
    "\n",
    "Вычитание среднего значения с учетом деления на стандартное отклонение не окажет влияние на перфоманс. Это можно показать следующим образом. Вычитание среднего значения и деление на стандартное отклонение даст нам дисперсию $\\sigma = 1$ и $\\mu = 0$, т.е. стандартное нормальное распределение $z = N(0,1)$. Следовательно расстояние L1 фактически просто масштабируется с фиксированным коэффициентом, ближайшие соседи не изменятся и, следовательно, перфоманс KNN классификатора останется прежней. Далее, если предположить, что\n",
    "$$\n",
    "||x^{(i)}-x^{(j)}||_{1} < ||x^{(i)}-x^{(k)}||_{1}\n",
    "$$\n",
    "то тогда дистанция с делением на стандартное отклонение можно переписать:\n",
    "$$\n",
    "||(x^{(i)} - \\bar{x}) / \\sigma - (x^{(j)} - \\bar{x}) / \\sigma||_1 = \\frac{1}{\\sigma}||x^{(i)} - x^{(j)}||_1 < \\frac{1}{\\sigma}||x^{(i)} - x^{(k)}||_1 = ||(x^{(i)} - \\bar{x}) / \\sigma - (x^{(k)} - \\bar{x}) / \\sigma||_1\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$.\n",
    "\n",
    "Перфоманс изменится. Так как мы делим на стандартное отклонение по пикселям, то соответственно мы и масштабируем разные измерения по-разному. Соответственно, ближайшие соседи для такого knn могут поменяться, что, в свою очередь, повлияет на перфоманс классификатора."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Rotating the coordinate axes of the data.\n",
    "\n",
    "Возможно тут стоит рассмотреть два случая.\n",
    "Например, если предполагаем, что все изображения поворачиваются одинаково, то в таком случае производительность не изменится.\n",
    "\n",
    "Но если рассматриваем поворот одной картинки, относительно другой, то таком случае перфоманс изменится. Для этого нужно рассмотреть три точки $x, y, z$ и посчитать расстояние L1 между ними (между $y$ и $x$ и между $y$ и $z$) и пусть они будут равны. Затем необходимо рассмотреть вращение координатных осей, соответственно получим новую матрицу поворота, например, на 15 или 45 градусов. Теперь, если посчитаем L1 расстояния, то расстояния между точками изменятся и не будут равны, что приводит к выводу, что порядок при вращении координатных осей для расстояния L1 не сохраняется. Следовательно и перфоманс меняется."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.\n",
    "1. The decision boundary (hyperplane between classes in feature space) of the k-NN classifier is linear.\n",
    "2. The training error of a 1-NN will always be lower than that of 5-NN.\n",
    "3. The test error of a 1-NN will always be lower than that of a 5-NN.\n",
    "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.\n",
    "5. None of the above.\n",
    "\n",
    "*Your Answer:* 2, 4\n",
    "\n",
    "\n",
    "*Your Explanation:*\n",
    "\n",
    "1. Неверно. Если мы возьмем евклидово расстояние в качестве меры, мы могли бы легко получить более гладкие границы решения, например обведенные кругом. Например, можно рассмотреть два класса в виде круга, где один окружает другой. Соответственно тогда граница решения будет иметь также примерно форму круга.\n",
    "\n",
    "2. Верно. Если мы будем использовать трейн-набор в качестве тест-набора данных, то тогда с одним ближайшим соседом при заданной точке x ближайший сосед будет точно такой же точкой, и, следовательно, ошибка будет равна 0. А для 5-NN 0 — это меньшая точка или нижняя граница.\n",
    "\n",
    "3. Неверно. Например, если рассмотреть размерность $dim = 1$. Возьмем пример в виде таких наборов точек: $x_{train}=(1,2,3,4,5)$  и  $y_{train}=(0,0,0,0,1)$. Тогда если рассмотреть новую точку с параметрами $x=0$ и $y=0$, то ошибка на тесте будет 100% для 1-NN, а для 5-NN равна 0%.  Также можно сделать вывод, что значение $k$ напрямую зависит от данных и $k$ необходимо подбирать для используемых данных, что дополнительно было показано в ДЗ 1, когда при разных k получали лучшую/худшую точность.\n",
    "\n",
    "4. Верно. Чтобы классифицировать точку, мы должны вычислить расстояние до других точек в обучающем наборе и найти k слоев. Больше обучающего набора -> Больше точек -> больше вычислений (набор данных, сортировки по дистанциям) -> требуется больше времени."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mipt",
   "language": "python",
   "name": "mipt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}